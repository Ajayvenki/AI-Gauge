# AI-Gauge VS Code Extension

Analyzes LLM API calls in your code and suggests cheaper model alternatives.

## ğŸš€ Quick Start (2 Minutes)

### 1. Install from VS Code Marketplace
```
Ctrl+Shift+X â†’ Search "AI-Gauge" â†’ Install â†’ Reload VS Code
```

### 2. That's it! âœ¨
AI-Gauge automatically:
- âœ… Installs Ollama (if missing)
- âœ… Downloads the AI-Gauge analysis model
- âœ… Configures everything automatically

### 3. Start Coding
Get instant cost optimization hints as you write LLM API calls!

---

## ğŸ¯ What It Does

AI-Gauge analyzes your code and provides real-time feedback on LLM model usage:

```python
# Your code:
response = client.chat.completions.create(
    model="gpt-4",  # âš ï¸ Overkill for simple tasks!
    messages=[...]
)

# AI-Gauge shows:
# ğŸ’¡ Switch to GPT-3.5-turbo â†’ Save 90% ($4.50 â†’ $0.45 per 1K calls)
```

---

## âœ¨ Features

### ğŸ” Smart Detection
- **Auto-Detection**: Finds OpenAI, Anthropic, Google, and custom API calls
- **Real-Time Analysis**: Analyzes as you type (optional)
- **Multi-Language**: Python, JavaScript, TypeScript support

### ğŸ’° Cost Optimization
- **Savings Alerts**: Shows potential cost reductions
- **Model Recommendations**: Suggests appropriate alternatives
- **Usage Tracking**: Monitors your API spending patterns

### ğŸŒ± Environmental Impact
- **Carbon Tracking**: Estimates COâ‚‚ footprint per API call
- **Green Suggestions**: Recommends efficient models
- **Sustainability Focus**: Helps reduce AI's environmental impact

### ğŸ¨ User Experience
- **Inline Hints**: Cost and latency indicators in your code
- **Quick Fixes**: One-click model replacement
- **Hover Details**: Detailed analysis on demand

---

## ğŸ› ï¸ Commands

- `AI-Gauge: Analyze Current File` - Analyze the active file
- `AI-Gauge: Analyze Workspace` - Analyze all supported files
- `AI-Gauge: Toggle Real-Time Analysis` - Enable/disable live analysis

---

## âš™ï¸ Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `aiGauge.enabled` | `true` | Enable/disable the extension |
| `aiGauge.realTimeAnalysis` | `false` | Analyze as you type |
| `aiGauge.showInlineHints` | `true` | Show inline cost hints |
| `aiGauge.costThreshold` | `20` | Min % savings to show hint |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VS Code Extension                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  extension.ts          - Main entry, registers providers     â”‚
â”‚  llmCallDetector.ts    - Detects LLM calls via regex/AST     â”‚
â”‚  aiGaugeClient.ts      - Calls local Ollama inference        â”‚
â”‚  diagnosticsProvider.ts - Shows warnings + quick fixes       â”‚
â”‚  inlineHintsProvider.ts - Shows inline cost/latency hints    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Local Ollama Inference                         â”‚
â”‚                  Fine-tuned Phi-3.5 Model                   â”‚
â”‚                                                              â”‚
â”‚  Model: ajayvenki01/ai-gauge                                 â”‚
â”‚  Runs: Locally on user machine                               â”‚
â”‚  Privacy: 100% local processing                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Detection Patterns

The extension detects LLM calls using intelligent patterns:

### OpenAI (Python)
```python
client.chat.completions.create(model="gpt-4o", ...)
client.beta.chat.completions.parse(model="gpt-4o-mini", ...)
```

### Anthropic (Python)
```python
client.messages.create(model="claude-3-opus", ...)
```

### Google (Python)
```python
model = genai.GenerativeModel("gemini-pro")
```

### OpenAI (JavaScript/TypeScript)
```javascript
const completion = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [...]
});
```

---

## ğŸ’¡ User Experience Examples

### Inline Hints (always visible):
```python
response = client.chat.completions.create(...)  # âš ï¸ $5.00/1k â€¢ slow â†’ ğŸ’¡ save 90%
```

### Diagnostics (squiggly underline):
- Yellow information squiggle on overkill model usage
- Hover for detailed analysis with reasoning

### Quick Fix (lightbulb):
- Click the lightbulb to replace model with recommended alternative
- Automatic code transformation

---

## ğŸ”§ Manual Setup (Advanced Users Only)

If auto-setup fails, you can manually configure:

### 1. Install Ollama
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

### 2. Pull AI-Gauge Model
```bash
ollama pull ajayvenki01/ai-gauge
```

### 3. Verify Installation
```bash
ollama list  # Should show ai-gauge model
```

---

## ğŸ› ï¸ Development

For extension developers:

### Prerequisites
- Node.js 16+
- VS Code 1.74+

### Setup
```bash
cd ide_plugin
npm install
npm run compile
```

### Development Commands
```bash
npm run watch      # Watch mode compilation
npm run compile    # One-time compilation
vsce package       # Create VSIX package
```

### Testing
- Open the extension in VS Code's Extension Development Host
- Test with files containing LLM API calls

---

## ğŸš€ Future Enhancements

- **Real API Interception**: Hook into actual API calls at runtime
- **Usage Analytics**: Track model usage patterns over time
- **Team Insights**: Aggregate cost savings across teams
- **Auto-Remediation**: Automatically optimize models in development
- **Multi-IDE Support**: Extend beyond VS Code

---

## ğŸ“Š Performance & Privacy

- **âš¡ Fast**: Local inference, no network latency
- **ğŸ”’ Private**: All analysis happens locally
- **ğŸ“± Offline**: Works without internet after setup
- **ğŸ§  Smart**: Fine-tuned Phi-3.5 model for accuracy
- **ğŸŒ Green**: Helps reduce AI's carbon footprint

---

**Ready to optimize your AI costs? Install AI-Gauge today!** ğŸš€

[Install from VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=Ajayvenki2910.ai-gauge)
