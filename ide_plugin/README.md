# AI-Gauge VS Code Extension

Analyzes LLM API calls in your code and suggests cheaper model alternatives using agent orchestration.

## ğŸš€ Quick Start (2 Minutes)

### 1. Install from VS Code Marketplace
```
Ctrl+Shift+X â†’ Search "AI-Gauge" â†’ Install â†’ Reload VS Code
```

### 2. That's it! âœ¨
AI-Gauge automatically:
- âœ… Copies bundled Python code to your local storage
- âœ… Installs Python dependencies
- âœ… Sets up Ollama for agent analysis
- âœ… Starts the inference server
- âœ… Configures everything automatically

### 3. Start Coding
Get instant cost optimization hints as you write LLM API calls!

---

## ğŸ¯ What It Does

AI-Gauge analyzes your code using a sophisticated agent pipeline and provides real-time feedback on LLM model usage:

```python
# Your code:
response = client.chat.completions.create(
    model="gpt-4",  # âš ï¸ Overkill for simple tasks!
    messages=[...]
)

# AI-Gauge shows:
# ğŸ’¡ Switch to GPT-3.5-turbo â†’ Save 90% ($4.50 â†’ $0.45 per 1K calls)
```

---

## âœ¨ Features

### ğŸ” Smart Detection
- **Auto-Detection**: Finds OpenAI, Anthropic, Google, and custom API calls
- **Real-Time Analysis**: Analyzes as you type (optional)
- **Multi-Language**: Python, JavaScript, TypeScript support

### ğŸ¤– Agent Orchestration
- **3-Agent Pipeline**: Metadata extraction, complexity analysis, and reporting
- **Local AI Integration**: Uses Ollama SLM within analyzer agent
- **Intelligent Recommendations**: Context-aware model suggestions

### ğŸ’° Cost Optimization
- **Savings Alerts**: Shows potential cost reductions
- **Model Recommendations**: Suggests appropriate alternatives based on task complexity
- **Usage Tracking**: Monitors your API spending patterns

### ğŸŒ± Environmental Impact
- **Carbon Tracking**: Estimates COâ‚‚ footprint per API call
- **Green Suggestions**: Recommends efficient models
- **Sustainability Focus**: Helps reduce AI's environmental impact

### ğŸ¨ User Experience
- **Inline Hints**: Cost and latency indicators in your code
- **Quick Fixes**: One-click model replacement
- **Hover Details**: Detailed analysis on demand

---

## ğŸ› ï¸ Commands

- `AI-Gauge: Analyze Current File` - Analyze the active file
- `AI-Gauge: Analyze Workspace` - Analyze all supported files
- `AI-Gauge: Toggle Real-Time Analysis` - Enable/disable live analysis

---

## âš™ï¸ Settings

| Setting | Default | Description |
|---------|---------|-------------|
| `aiGauge.enabled` | `true` | Enable/disable the extension |
| `aiGauge.showInlineHints` | `true` | Show inline cost hints |
| `aiGauge.costThreshold` | `20` | Min % savings to show hint |
| `aiGauge.modelServerUrl` | `http://localhost:8080` | Inference server URL |
| `aiGauge.serverAutoStart` | `true` | Automatically start inference server |
| `aiGauge.serverHealthCheckInterval` | `30` | Health check interval (seconds) |

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    VS Code Extension                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  extension.ts          - Main entry, server lifecycle mgmt   â”‚
â”‚  llmCallDetector.ts    - Detects LLM calls via regex/AST     â”‚
â”‚  aiGaugeClient.ts      - Communicates with inference server  â”‚
â”‚  diagnosticsProvider.ts - Shows warnings + quick fixes       â”‚
â”‚  inlineHintsProvider.ts - Shows inline cost/latency hints    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Inference Server (Flask)                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ REST API for extension communication                      â”‚
â”‚  â€¢ Agent orchestration via LangGraph                        â”‚
â”‚  â€¢ Health monitoring and automatic recovery                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Decision Module (LangGraph)                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Agent 1: Metadata Extractor - Analyzes call patterns       â”‚
â”‚  Agent 2: Analyzer (Ollama SLM) - Assesses task complexity   â”‚
â”‚  Agent 3: Reporter - Generates cost-saving recommendations  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Model Cards Database                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â€¢ Single source of truth for model metadata                â”‚
â”‚  â€¢ Tiers, costs, carbon factors, performance data           â”‚
â”‚  â€¢ Used by all agents for business logic                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Detection Patterns

The extension detects LLM calls using intelligent patterns:

### OpenAI (Python)
```python
client.chat.completions.create(model="gpt-4o", ...)
client.beta.chat.completions.parse(model="gpt-4o-mini", ...)
```

### Anthropic (Python)
```python
client.messages.create(model="claude-3-opus", ...)
```

### Google (Python)
```python
model = genai.GenerativeModel("gemini-pro")
```

### OpenAI (JavaScript/TypeScript)
```javascript
const completion = await openai.chat.completions.create({
  model: "gpt-4",
  messages: [...]
});
```

---

## ğŸ’¡ User Experience Examples

### Inline Hints (always visible):
```python
response = client.chat.completions.create(...)  # âš ï¸ $5.00/1k â€¢ slow â†’ ğŸ’¡ save 90%
```

### Diagnostics (squiggly underline):
- Yellow information squiggle on overkill model usage
- Hover for detailed analysis with reasoning

### Quick Fix (lightbulb):
- Click the lightbulb to replace model with recommended alternative
- Automatic code transformation

---

## ğŸ”§ Manual Setup (Advanced Users Only)

If auto-setup fails, you can manually configure:

### 1. Install Python Dependencies
```bash
pip install -r requirements.txt
```

### 2. Set Up Ollama (for Agent Analysis)
```bash
# Install Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# Start Ollama service
ollama serve

# Pull required model (handled automatically by server)
ollama pull llama3.2:3b
```

### 3. Start Inference Server
```bash
python src/inference_server.py
```

### 4. Verify Installation
```bash
# Check server health
curl http://localhost:8080/health

# Should return: {"status":"ok","agents":"ready","ollama":"connected"}
```

---

## ğŸ› ï¸ Development

For extension developers:

### Prerequisites
- Node.js 16+
- VS Code 1.74+

### Setup
```bash
cd ide_plugin
npm install
npm run compile
```

### Development Commands
```bash
npm run watch      # Watch mode compilation
npm run compile    # One-time compilation
vsce package       # Create VSIX package
```

### Testing
- Open the extension in VS Code's Extension Development Host
- Test with files containing LLM API calls

---

## ğŸš€ Future Enhancements

- **Real API Interception**: Hook into actual API calls at runtime
- **Usage Analytics**: Track model usage patterns over time
- **Team Insights**: Aggregate cost savings across teams
- **Auto-Remediation**: Automatically optimize models in development
- **Multi-IDE Support**: Extend beyond VS Code

---

## ğŸ“Š Performance & Privacy

- **âš¡ Smart**: Agent orchestration provides context-aware analysis
- **ğŸ”’ Private**: All analysis happens locally on your machine
- **ğŸ“± Offline**: Works without internet after initial setup
- **ğŸ§  Intelligent**: Multi-agent pipeline with local AI integration
- **ğŸŒ Green**: Helps reduce AI's carbon footprint through optimization
- **ğŸ”„ Automatic**: Server lifecycle managed by extension
- **ğŸ’ª Reliable**: Health checks and automatic recovery

---

**Ready to optimize your AI costs with agent-powered analysis? Install AI-Gauge today!** ğŸš€

[Install from VS Code Marketplace](https://marketplace.visualstudio.com/items?itemName=Ajayvenki2910.ai-gauge)
